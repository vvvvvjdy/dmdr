<h1 align="center"> DMDR:<br>Distribution Matching Distillation Meets  Reinforcement Learning</h1>
<div align="center">
  <a href='https://arxiv.org/abs/2511.13649'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;
  <a href="https://github.com/vvvvvjdy/dmdr"><img src="https://img.shields.io/badge/GitHub(DMDR)-9E95B7?logo=github"></a> &nbsp;
</div>


---

![our samples](assets/demo_pic_z.png)


<p align="center">
   <figcaption style="text-align: center; margin-top: 10px; font-size: 0.95em;">
            DMDR contains three key elements: (1). A DMD branch to optimize the generator using the gradient of an implicit distribution matching objective;  
            (2). A RL branch to synchronously introduce the reward feedback from the reward model;  
            (3). Two dynamic training strategies to achieve faster and better distillation in the initial phase.
        </figcaption>
  < img src="assets/dmdr_method.png" alt="method" style="width:100%;">
</p >

---


## ðŸ”¥ Note

Our training and inference code is currently undergoing internal check of the company. Once it passes, we will fully open source it.



<p align="center">
   <figcaption style="text-align: center; margin-top: 10px; font-size: 0.95em;">
            Images generated by SD3.5 Large finetuned through DMDR with open-source data and reward model using  4 NFE.
        </figcaption>
  <img src="assets/demo_pic_sd35.png" alt="Result" style="width:92%;">
</p>




## ðŸŒº Citation
If you find DMDR useful, please kindly cite our paper:
```bibtex
@article{jiang2025dmdr,
title={Distribution Matching Distillation Meets Reinforcement Learning},
author={Jiang, Dengyang and Liu, Dongyang and Wang, Zanyi and Wu, Qilong and Jin, Xin and Liu, David and Li, Zhen and Wang, Mengmeng and Gao, Peng and Yang, Harry},
journal={arXiv preprint arXiv:2511.13649},
year={2025}
}
```
